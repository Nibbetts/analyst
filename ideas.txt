Overall:
--------
- build in anvaka's visualization?
- fix all to work in python3 - check if evaluator kwargs need to be unicode,
particularly if run and save in python 2, then go to 3 to run again.
!Protocol level won't allow this?
- test imports for unnecessary ones?
- documentation
- update metric_args to use a partial function instead, from functtools?
Fix problem where metric_args doesn't work on non-scipy-builtins.
Sholdn't interfere with scipy because of metric_str? Use update_wrapper, python3 book pg 145.
Or if not functtools.partial, then functtools.partialmethod, or simple decorator.
- use built-in print function's file-handler to output to file?

Analyses:
---------
- check if there are properties associated with length of vectors in non-normalized spaces,
ex: check if outliers are longer
- on presentable analysis make sure that same words in each space, and or trained
on same training data, maybe as separate analysis!
- fasttext is not normalized - compare to self normalized!
- compare before adn after being in an RNN. Norms, and also performance on analogies.
- find which stats are most indicative of success, then add those into a loss function!!
- various hub configurations side by side
- (my, in purple notebook) outlier count in small space vs large, and node count.

Analyst:
--------
- make function to compare clustering or analogy algorithms in a given analyst
(can be done in post-processing of text until this becomes relevant)
compare_categories, side by side.
- add in_frequency_order=False and frequencies=[] to analyst, complete with tests on that.
- list words who are no one's nearest neighbor
- decide if multi-comparison needs grapher....
- make grapher object, and have analyst have it as a data member - analyst's graph
function just call's its grapher's graph function.
- make delete_evaluator function, and function to make it printable if not?
or make it so that adding, when not a category for it, won't replace it?
- continue adding make_default_evaluator options for new evaluators.
- add functionality to recompute stars and data but not clusters, or give disclaimer/
warning that changing it won't work.
- evaluation indicators to test validity of an algorithm?
Davies-Bouldin, Dunn, Silhouette coefficient; or for external evaluation:
Rand indicator, F indicator, Jaccard indicator,Fowlkes-mallows indicator,
Mutual information, Confusion matrix.

Word Group Specific Analysis:
-----------------------------
- make a scholar evaluator?
- make it so clusters only compute at start what is needed for report - other
info can come in an inspection function! not printed when cluster printed.
- make funct to examine a group of given strings thoroughly - makes a cluster.
adds it to where?
- look into cluster extremities! (strange attractors)
- check if correct standard deviation of vectors from centroids - like dispersion:
https://math.stackexchange.com/questions/2442132/generalize-standard-deviation-to-vectors
(This is also for the space as a whole)
- use ward dissimilarity somehow? In inspection of a given group, dissimilarity to
all other clusters?
- make second_downstream function? And maybe one that switches
randomly if dist within epsilon

Clusterizers TO DO:
-------------------
- compute both nodal and branching hubs at same time instead of redoing?
- cluster by length of vector to test?
- Spatial
    - include Local Outlier Factor, Local Reachability Density.
    Perhaps in separate clusterizer?
    - covariance, variance, 
- Rest of built-ins
- EM clustering (base default off of nodal hub count, or allow varying parameters)
- make kmeans start from hub centroids, and base on hub count?
- PHATE (base default off of nodal hub count, or allow varying parameters)
- pick some other important ones, affinity?
- Rule-based partitioning clusters (see purple notebook)
- implement a k-means cheat clusterizer in which you pick words to be cluster centers,
and partition the rest into those. ex: give specific example use cases for clusters,
such as having a custom clustering algorithm which builds clusters stemmed from
color words. include as example



EMBEDDING ALGORITHM IDEAS
-------------------------
- keep a frequency value for each word, and only shift by weighted amounts as we train.


(categories:)
spatial
nodes
clusters
user_cluster_algorithm1
user_cluster_algorithm2
#General Stats on analogy datasets:
#    set1
#    set2
#    set3
Stats on Analogy Set1:
    dispersion
    medoid
    etc
    similarity to set2
    similarity to set3
Stats on Analogy set2:
    ...
Stats on Analogy set3:
    ...
Analogy Algorithm 1:
    set1_score
    set2_score
    set3_score
Reclustering stats based on analogy test sets:
    (new general space info based on clusters)



average distance to nearest; nearest n
average distance to furthest; furthest n
average distance

find words where nearest are each other (these are nodes),
    then recursively find all words whose nearest are one of these, and so on.
    This should find peaks, or local concentrations; turn these into clusters.
    note: may end up with nearly num_words/2 clusters
    print percent of words that are part of a node
find average distance between words in "clusters".
    Some nodes will be closer than this, and to more than one cluster.
    But if we add all, may end up with one giant cluster.
    May label those further than this as outliers?
    Use avg dist between words in node-pairs as cutoff for outliers instead of avg dist of that cluster?
    Make new entity called "core" of all within that distance; primary dispersion measurement on this? Or would this simply be like the same as avg node width?
make function to print primary components of cluster; nodes/those closest.

make function to take in word/vec list and make a cluster based on it,
    then find all other words likely to belong to this cluster.
    This is useful since certain types of clusters will frequently overlap and words will belong to multiple classes.
    Then can measure purity of cluster based on user feed-back of what words didn't belong.
    Could use this feedback to analyze important dimensional components of cluster; where to extend and where to stop;
    can also measure extremity of cluster before and after this, representing misshapenness.

Hierarchicality:
    nodal factor/shattered factor: num words that are part of a node divided by num not
    hierarchical facter (burst factor is 1-this or 1/this?):
        num nodes that are part of supernodes versus num not

Have each node store a vector representation of the line from a to b,
    normalize them, then compute an average abs(dot product) of these,
    as a measure of alignment of nodes.
    Get some sort of distribution graph of node lengths, like we did with word distances.


wordalysis
embedology
vectorology
vectoralysis



try projecting each word vector onto its major axis and see how they are related. (Darian's Idea)
