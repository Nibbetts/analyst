nate@hatch:~/Projects/analyst$ python
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import analyst
>>> a = analyst.Analyst.load("/home/nate/Projects/an_scholar4000_real.dill")


write analogical analyses to compare:
glove vectors
FastText
word2vec
conceptnet numberbatch


There aren't regions in the space, per se. If things are often around 90 degrees
apart, then they are dispersed well, as like 6 points across the surface of a ball.


Overall:
--------
- fix all to work in python3
- test imports
- documentation

Analyses:
---------
- compare google's universal sentence encoder used on words with fastext etc
- various hub configurations side by side
- (my, in purple notebook) outlier count in small space vs large, and node count.

Analyst:
--------
- make capacity to find neighbors of nonexistent vector!
- phase out 1,2,f and make K neighbor calculator. change asserts to reflect it.
- make printer recheck data without recalculating in case things have changed?
- fill in make_default_evaluator
- add functionality to recompute stars and data but not clusters, or give disclaimer/
warning that changing it won't work.
- add functionality to be able to add clusterizers and compute without recomputing,
unless recompute_all=True.
- call compute on clusterizers, and give also node info etc.
- fix encoder/decoder initialization to see what can/can't be None, and force
what will be required.
- evaluation indicators to test validity of an algorithm?
Davies-Bouldin, Dunn, Silhouette coefficient; or for external evaluation:
Rand indicator, F indicator, Jaccard indicator,Fowlkes-mallows indicator,
Mutual information, Confusion matrix.
- implement grapher

Word Group Specific Analysis:
-----------------------------
- make downstream function, second_downstream function, maybe one that switches
randomly if dist within epsilon
- use ward dissimilarity somehow? In inspection of a given group, dissimilarity to
all other clusters?
- look into cluster extremities! (strange attractors)
- make funct to examine a group of given strings thoroughly
- standard deviation of vectors from centroids - like dispersion:
https://math.stackexchange.com/questions/2442132/generalize-standard-deviation-to-vectors
(This is also for the space as a whole)

Clusterizers TO DO:
-------------------
- instead of specific stats, allow variety of stats in each cluster, with dict,
and iterate through in cluterizer that adds info.
- Spatial
    - include Local Outlier Factor, Local Reachability Density.
    Perhaps in separate clusterizer?
    - covariance, variance, 
- Rest of built-ins
- kmeans or EM (base default off of nodal hub count, or allow varying parameters)
- PHATE (base default off of nodal hub count, or allow varying parameters)
- pick some other important ones, affinity?
- use variable density of space, by measuring the density of the surrounding region,
say, pick 10 nearest, and measure their dispersion or repulsion or something; then
can use this for clusterizing. Could even compare nearer 4 to further 8 or something
and take a derivative to know how fast the density is changing!
Want to only define a cluster from a node if local density is decreasing as we go out.
However, this may have exactly the same effect as defining a hub, because all of the
nearest neighbor arrows are pointing in - which means density will decrease as we go out.
Thus hubs have a mathematical basis.
- Rule-based partitioning clusters (see purple notebook)
- implement a k-means cheat clusterizer in which you pick words to be cluster centers,
and partition the rest into those.
- give specific example use cases for clusters, such as having a custom clustering
algorithm which builds clusters stemmed from color words. include as example
- check that we're only printing what we should - particularly with nodes?

Analogical Analyses:
--------------------
- EVERYTHING
- make clusterizer, analogist, and analogyRanker subclasses of evaluator.


In [121]: def second_downstream(word):
     ...:     l = []
     ...:     n = word
     ...:     while n not in l:
     ...:         l.append(n)
     ...:         n = a.second_nearest(n)
     ...:     print l



(categories:)
spatial
nodes
clusters
user_cluster_algorithm1
user_cluster_algorithm2
#General Stats on analogy datasets:
#    set1
#    set2
#    set3
Stats on Analogy Set1:
    dispersion
    medoid
    etc
    similarity to set2
    similarity to set3
Stats on Analogy set2:
    ...
Stats on Analogy set3:
    ...
Analogy Algorithm 1:
    set1_score
    set2_score
    set3_score
Reclustering stats based on analogy test sets:
    (new general space info based on clusters)



average distance to nearest; nearest n
average distance to furthest; furthest n
average distance

find words where nearest are each other (these are nodes),
    then recursively find all words whose nearest are one of these, and so on.
    This should find peaks, or local concentrations; turn these into clusters.
    note: may end up with nearly num_words/2 clusters
    print percent of words that are part of a node
find average distance between words in "clusters".
    Some nodes will be closer than this, and to more than one cluster.
    But if we add all, may end up with one giant cluster.
    May label those further than this as outliers?
    Use avg dist between words in node-pairs as cutoff for outliers instead of avg dist of that cluster?
    Make new entity called "core" of all within that distance; primary dispersion measurement on this? Or would this simply be like the same as avg node width?
make function to print primary components of cluster; nodes/those closest.

make function to take in word/vec list and make a cluster based on it,
    then find all other words likely to belong to this cluster.
    This is useful since certain types of clusters will frequently overlap and words will belong to multiple classes.
    Then can measure purity of cluster based on user feed-back of what words didn't belong.
    Could use this feedback to analyze important dimensional components of cluster; where to extend and where to stop;
    can also measure extremity of cluster before and after this, representing misshapenness.

Hierarchicality:
    nodal factor/shattered factor: num words that are part of a node divided by num not
    hierarchical facter (burst factor is 1-this or 1/this?):
        num nodes that are part of supernodes versus num not

Have each node store a vector representation of the line from a to b,
    normalize them, then compute an average abs(dot product) of these,
    as a measure of alignment of nodes.
    Get some sort of distribution graph of node lengths, like we did with word distances.

embedysis
encodysis
wordalysis
embedology
vectorology
vectoralysis



try projecting each word vector onto its major axis and see how they are related. (Darian's Idea)



def shift(string, k=1):
    # Usefuls start at 32 and end at 126. So mod by 127-32=95.
    s = ""
    for c in string:
        n = ord(c)
        if n >= 32 and n <= 126:
            s += chr((n-32 + k) % 95 + 32)
        else: s += c
    return s


"""
if len(self.space) >= 4:
    #return
    for i, vec in enumerate(tqdm(self.space, disable=(not self.auto_print),
                            desc="Acquainting the Species")):
        #nearest_i = (0 if i != 0 else 1) # Can't start off on self!
        #nearest_2i = (2 if i != 2 else 3) # Can't start off same as nearest!
        nearest_i = (i + 1) % len(self.space)
        nearest_2i = (i + 2) % len(self.space)
        furthest_i = i # Start off closest possible - self.
        nearest_dist = self.metric(vec, self.space[nearest_i])
        nearest_2dist = self.metric(vec, self.space[nearest_2i])
        furthest_dist = 0
        # In case we started them off switched:
        if nearest_2dist < nearest_dist:
            temp_i = nearest_i
            temp_dist = nearest_dist
            nearest_i = nearest_2i
            nearest_2i = temp_i
            #furthest_i = temp_i
            nearest_dist = nearest_2dist
            nearest_2dist = temp_dist
            #furthest_dist = temp_dist
        for j, other in enumerate(self.space):
            if j != i:
                dist = self.metric(vec, other)
                if dist < nearest_dist:
                    nearest_2dist = nearest_dist
                    nearest_2i = nearest_i
                    nearest_dist = dist
                    nearest_i = j
                elif dist < nearest_2dist and j != nearest_i:
                    nearest_2dist = dist
                    nearest_2i = j
                if dist > furthest_dist:
                    furthest_dist = dist
                    furthest_i = j
        self.neighbors[i][0] = nearest_i
        self.neighbors[i][1] = nearest_2i
        self.neighbors[i][2] = furthest_i
        self.neighbors_dist[i][0] = nearest_dist
        self.neighbors_dist[i][1] = nearest_2dist
        self.neighbors_dist[i][2] = furthest_dist

        #for testing only:
        #if i > 10: break"""