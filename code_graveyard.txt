A Place Where Deprecated Code Goes to Die...
--------------------------------------------------------------------------------    

// FROM SPATIALIZER
        # Compute the rest and add all stats to stats_dict:
        self.stats_dict["Count"] = len(space)
        if len(space) > 0:
            # Overall Info:
            self.stats_dict["Dimensionality"] = len(space[0])
            printer("Electing a Ruler", "Getting Medoid, Std Dev, Skew")
            self.stats_dict["Medoid - Obj Nearest to Centroid"] = cluster.medoid
            self.stats_dict["Dist from Medoid to Centroid"] = cluster.medoid_dist
            self.stats_dict["Standard Dev"] = cluster.std_dev
            if self.node_clusterizer != None:
                self.stats_dict["Skew"] = cluster.skew
                # Note: not bothering with numbers of nodes etc, since that is
                #   reported by the NodeClusterizer.

            # Centroid Info:
            printer("Setting Priorities", "Centroid Stats")
            self.stats_dict["Centroid Norm"] = cluster.centroid_norm
            self._compute_list_stats(cluster.centroid_distances,
                "Centroid Dist", self.stats_dict) # Then re-key one entry:
            dispersion = self.stats_dict.pop("Centroid Dist Avg")
            self.stats_dict["Dispersion - Centroid Dist Avg"] = dispersion

// FROM CLUSTERIZER
        self.stats_dict["Count"] = len(self.clusters)
        if len(self.clusters) > 0:
            self._compute_list_stats([len(c) for c in self.clusters],
                "Population",  self.stats_dict)
            self._compute_list_stats([c.centroid_norm \
                for c in self.clusters], "Centroid Norm", self.stats_dict)
            self._compute_list_stats([c.dispersion for c in self.clusters],
                "Dispersion",  self.stats_dict)
            self._compute_list_stats([c.std_dev    for c in self.clusters],
                "Standard Dev", self.stats_dict)
            self._compute_list_stats([c.repulsion  for c in self.clusters],
                "Repulsion",   self.stats_dict)
            if self.node_clusterizer != None:
                self._compute_list_stats([c.skew       for c in self.clusters],
                    "Skew",        self.stats_dict)
                self._compute_list_stats([len(c.nodes) for c in self.clusters],
                    "Node Count",  self.stats_dict)


//FROM ANALYST:

            # analogy_algorithms -- list of tuples (callable, "Description").
            #     Each callable must take word_A, is_to_B, as_C, and must return
            #     is_to_D.
            #     NOTE: As this variable contains functions, it will be altered
            #         to no longer contain functions when an Analyst is pickled.
            # analogy_sets -- list of tuples: (list_of_quadruples, "Description),
            #     where each quadruple is a list of four strings, D being the
            #     correct answer to compare against. Example:
            #     [
            #         (
            #             [
            #                 ["wordA", "wordB", "wordC", "wordD"],
            #                 ["wordA", "wordB", "wordC", "wordD"]
            #             ],
            #             "Analogy Test Set 1"
            #         ),
            #         (
            #             [
            #                 ["wordA", "wordB", "wordC", "wordD"],
            #                 ["wordA", "wordB", "wordC", "wordD"]
            #             ],
            #             "Analogy Test Set 2"
            #         )
            #     ]

    # Compare this analyst with another, data per data.
    def compare_difference(self, analyst2, w=10, comparator=u"simple"):
        # Prints a full report with three numbers for each property
        #   instead of one - val_for_A, val_for_B, A_B_compared.
        # comparator:
        #   callable (not used on strings or None), or a built-in:
        #   "none" or None: don't print the third column
        #   "simple": A - B
        #   "weighted": (A - B) / avg(abs(A), abs(B))
        # w: Numbers will have space for w-2 digits, (w-2 because of . and - ).
        #   Total width will be: (3 + (w + 1)*num_cols + 2 + len(description))
        # Returns: a grapher object with double-histogram information
        #   from the comparison
        self._print(u"Bridging Two Universes",
            u"Building One-to-one Comparison")
        print(u"")
        
        # Descriptions to use:
        if self.description == None: desc = u"ANALYST 1"
        else: desc = self.description
        if analyst2.description == None: desc2 = u"ANALYST 2"
        else: desc2 = analyst2.description
        print(desc.upper() + u" vs. " + desc2.upper())

        # Comparator:
        if callable(comparator): comparison = comparator
        elif comparator == None or comparator.lower() == "none":
            comparator = None
            comparison = lambda a, b: ""
        elif comparator.lower() == u"simple": comparison = lambda a, b: a - b
        else: comparison = Analyst.weighted_difference

        # Combine and sort the Categories without losing any of them:
        all_categories = []
        category_indeces = {}
        # From left analyst (self):
        for i, category in enumerate(self.categories):
            if category not in all_categories:
                all_categories.append(category)
                category_indeces[category] = (i, None)
            else:
                t = category_indeces[category]
                if t[0] == None: category_indeces[category] = (i, t[1])
        # From right analyst (other):
        for i, category in enumerate(analyst2.categories):
            if category not in all_categories:
                all_categories.append(category)
                category_indeces[category] = (None, i)
            else:
                t = category_indeces[category]
                if t[1] == None: category_indeces[category] = (t[0], i)        

        # Combine and sort the info in each category without losing any:
        for category in all_categories:
            print(category + u": ")
            # Gather info from each:
            try:
                info_list_1 = self.category_lists[category_indeces[category][0]]
            except: info_list_1 = []
            try:
                info_list_2 = analyst2.category_lists[
                    category_indeces[category][1]]
            except: info_list_2 = []

            all_info = []
            info_indeces = {}
            # From left analyst (self):
            for i, info in enumerate(info_list_1):
                if info[0] not in all_info:
                    all_info.append(info[0])
                    info_indeces[info[0]] = (i, None)
                else:
                    t = info_indeces[info[0]]
                    if t[0] == None: info_indeces[info[0]] = (i, t[1])
            # From right analyst (other):
            for i, info in enumerate(info_list_2):
                if info[0] not in all_info:
                    all_info.append(info[0])
                    info_indeces[info[0]] = (None, i)
                else:
                    t = info_indeces[info[0]]
                    if t[1] == None: info_indeces[info[0]] = (t[0], i)    

            # Then for the combined info from that category:
            for info in all_info:
                # Combine each line of info, checking for None:
                # comb = [description, var1, var2, comparison, star]
                info1 = (info_list_1[info_indeces[info][0]] if
                    info_indeces[info][0] != None else None)
                info2 = (info_list_2[info_indeces[info][1]] if
                    info_indeces[info][1] != None else None)
                if info1 == info2 == None:
                    comb = [u"???", None, None, u"", u" "]
                elif info1 == None:
                    comb = [info2[0], None, info2[1], u"",
                        u"*" if info2[2] else u" "]
                elif info2 == None:
                    comb = [info1[0], info1[1], None, u"",
                        u"*" if info1[2] else u" "]
                else: comb = [info1[0], info1[1], info2[1], u"",
                    u"*" if (info1[2] or info2[2]) else u" "]
                
                is_hist = u"Histogram Key" in comb[0]

                # Compute comparison:
                if comb[1] != None and comb[2] != None:
                    if u"Histogram Key" in comb[0]:
                        # Add a new key for a combined histogram:
                        self.graph_info.append((u"c", info1[1], info2[1]))
                        comb[3] = len(self.graph_info) - 1
                    elif not (isstring(comb[1]) or isstring(comb[2])):
                        comb[3] = comparison(comb[1], comb[2])
                
                # Formatting:
                comb[1] = Analyst._formatit(comb[1], w, is_hist, 3)
                comb[2] = Analyst._formatit(comb[2], w, is_hist, 3+(w+1))
                comb[3] = Analyst._formatit(comb[3], w, is_hist, 3+(w+1)*2)

                # And finally print a line:
                if comparator == None:
                    print(u"  {}{} {} {} {}".format(
                        comb[4], comb[1], comb[2], comb[4], comb[0]))
                else: print(u"  {}{} {} {} {} {}".format(
                    comb[4], comb[1], comb[2], comb[3], comb[4], comb[0]))

    # # Nuclei:
    # if "Nuclei" in self.categories:
    #     self.nuclei = clusters.clusterizer.compute_nuclei()
    #     self._print("Performing Cold Fusion")
    #     self._add_cluster_type_attributes(self.nuclei, "Nuclei")

    # # Anti-hubs:
    # if "Anti-hubs" in self.categories:
    #     self.anti_hubs = clusters.clusterizer.compute_anti_hubs()
    #     self._print("Unraveling the Secrets of Dark Matter")
    #     self._add_cluster_type_attributes(self.anti_hubs, "Anti-hubs")





self.strings = None
if callable(encoder) or encoder is None:
    self.encode = encoder # string to vector
    self.decode = decoder # vector to string
else:
    assert len(encoder) == len(self.space)
    self.strings = encoder
    self.vec_to_s = {}
    self.s_to_vec = {}
    for i in range(len(self.space)):
        self.vec_to_s[self.space[i]] = self.strings[i]
        self.s_to_vec[self.strings[i]] = self.space[i]
    self.encode = self.s_to_vec.__getitem__
    self.decode = self.vec_to_s.__getitem__




"""
def _serialize(self):
    # method to prepare an analyst instance for saving.
    if not self.serialized:
        self.metric = self.metric.__name__ if self.metric != None else None
        self.encode = self.encode.__name__ if self.encode != None else None
        self.decode = self.decode.__name__ if self.decode != None else None
        self.cluster_algorithms = [
            (None, pair[1]) for pair in self.cluster_algorithms]
        self.analogy_algorithms = [
            (None, pair[1]) for pair in self.analogy_algorithms]

        # Serialize the Clusters (Nodes don't need it):
        for key in self.cluster_data:
            for cluster in self.cluster_data[key]:
                cluster._serialize()

        self.serialized = True

def _deserialize(self, metric, encoder, decoder,
                    cluster_algorithms, analogy_algorithms):
    assert self.serialized
    if callable(metric):
        assert metric.__name__ == self.metric
        self.metric = metric
    elif metric == "l2" or metric == "euclidean":
        self.metric = sp.distance.euclidean
    elif metric == "cosine_similarity":
        self.metric = sp.distance.cosine
    elif metric == "l1":
        self.metric = sp.distance.cityblock
    else: raise ValueError("'metric' parameter unrecognized and uncallable")            
        
    if encoder != None: assert encoder.__name__ == self.encode
    self.encode = encoder
    if decoder != None: assert decoder.__name__ == self.decode
    self.decode = decoder

    if cluster_algorithms != None:
        assert zip(*cluster_algorithms)[1] == zip(*self.cluster_algorithms)[1]
    self.cluster_algorithms = cluster_algorithms
    if analogy_algorithms != None:
        assert zip(*analogy_algorithms)[1] == zip(*self.analogy_algorithms)[1]
    self.analogy_algorithms = analogy_algorithms

    # Deserialize the Clusters:
    for key in self.cluster_data:
        for cluster in self.cluster_data[key]:
            cluster._deserialize(metric, encoder, decoder)

    self.serialized = False
"""

"""
if len(self.space) >= 4:
    #return
    for i, vec in enumerate(tqdm(self.space, disable=(not self.auto_print),
                            desc="Acquainting the Species")):
        #nearest_i = (0 if i != 0 else 1) # Can't start off on self!
        #nearest_2i = (2 if i != 2 else 3) # Can't start off same as nearest!
        nearest_i = (i + 1) % len(self.space)
        nearest_2i = (i + 2) % len(self.space)
        furthest_i = i # Start off closest possible - self.
        nearest_dist = self.metric(vec, self.space[nearest_i])
        nearest_2dist = self.metric(vec, self.space[nearest_2i])
        furthest_dist = 0
        # In case we started them off switched:
        if nearest_2dist < nearest_dist:
            temp_i = nearest_i
            temp_dist = nearest_dist
            nearest_i = nearest_2i
            nearest_2i = temp_i
            #furthest_i = temp_i
            nearest_dist = nearest_2dist
            nearest_2dist = temp_dist
            #furthest_dist = temp_dist
        for j, other in enumerate(self.space):
            if j != i:
                dist = self.metric(vec, other)
                if dist < nearest_dist:
                    nearest_2dist = nearest_dist
                    nearest_2i = nearest_i
                    nearest_dist = dist
                    nearest_i = j
                elif dist < nearest_2dist and j != nearest_i:
                    nearest_2dist = dist
                    nearest_2i = j
                if dist > furthest_dist:
                    furthest_dist = dist
                    furthest_i = j
        self.neighbors[i][0] = nearest_i
        self.neighbors[i][1] = nearest_2i
        self.neighbors[i][2] = furthest_i
        self.neighbors_dist[i][0] = nearest_dist
        self.neighbors_dist[i][1] = nearest_2dist
        self.neighbors_dist[i][2] = furthest_dist

        #for testing only:
        #if i > 10: break"""



--------------------------------------------------------------------------------
NOT DEPRECATED, BUT UNSURE AND HOMELESS:
--------------------------------------------------------------------------------

# Specific Functions:
def rescale(self, theta, alpha=15, power=0.5):
    ''' Rescales based on observed distribution of angles between words
        in a postagged Wikipedia word embedding from BYU PCCL.
        Accepts theta in radians.'''
    return (0.5 + (math.atan((theta*180/np.pi - 90)/alpha)
                    / (2*math.atan(90/alpha))))**power

def test_angles(self, n, alpha=15, power=0.5):
    dist = [self.rescale(self.s.angle(
                self.s.get_vector(self.s.model.vocab[int(x)]),
                self.s.get_vector(self.s.model.vocab[int(2*x)])),
                alpha, power)
            for x in (np.random.random(n)*len(self.s.model.vocab)/2.0)]
    plt.hist(dist, 90)
    plt.show()

#def scale_bimodal(self, theta):
#    deg = theta*180/np.pi
#    return 0.5 + (self.cbrt((deg-90)) / (2*self.cbrt(90)))

def cluster_analogy(self, A, B, C, AC_clustername, B_clustername,
                    num_words=1, exclude=True):
    ''' Follows form: A:B::C:D.
        Assumes that we know which cluster each word comes from.'''
    dist = self.s.get_angle(A, B)
    A_tighter = (self.clusters[AC_clustername][1]
                <= self.clusters[B_clustername][1]
    C_vec = self.s.get_vector(C)
    dir_vec = self.clusters[AC_clustername][0] - C_vec
    if A_tighter: dir_vec = -dir_vec
    D_vec = self.s.yarax(C_vec, dir_vec, dist)
    D_vec /= np.linalg.norm(D_vec)

    if exclude:
        if self.s.slim == True: # This branch other part of patch:
            results = self.s.wordify(
                self.s.model.get_closest_words(D_vec, num_words+3))
            trimmed = ([word for word in results[0]
                        if word not in [A, B, C]],
                    [results[1][i] for i in range(len(results[1]))
                        if results[0][i] not in [A, B, C]])
            return (np.array(trimmed[0][:num_words:]),
                    np.array(trimmed[1][:num_words:]))
        else: # This branch is the original return:
            return self.s.wordify(self.s.model.get_closest_words_excluding(
                D_vec, [self.s.get_vector(A), self.s.get_vector(B), C_vec],
                num_words))
    else: # The real original return...
        return self.s.wordify(
            self.s.model.get_closest_words(D_vec, num_words))

def divergence_analogy(self, A, B, C):
''' Automatically tries to find clusters around A and B,
    and then does a cluster analogy.'''
raise NotImplementedError("Function not implemented.")

